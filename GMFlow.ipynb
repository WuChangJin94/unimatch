{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zj3mH_CY8s9x",
    "outputId": "10ef4222-d6e1-4fa9-cf85-33110b0f8b2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0+cu118\n",
      "Running on GPU NVIDIA GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "# check env\n",
    "import torch\n",
    "\n",
    "print('PyTorch version: %s' % torch.__version__)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "  print('Running on CPU')\n",
    "else:\n",
    "  print('Running on GPU %s' % torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ABlla2TL5G_2"
   },
   "outputs": [],
   "source": [
    "# visualization utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (30, 5)\n",
    "\n",
    "def vis_img(img):\n",
    "  plt.imshow(img)\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()\n",
    "\n",
    "def vis_video(video_path):\n",
    "  mp4 = open(video_path,'rb').read()\n",
    "  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "  return HTML(\"\"\"\n",
    "  <video width=600 controls>\n",
    "        <source src=\"%s\" type=\"video/mp4\">\n",
    "  </video>\n",
    "  \"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLhfzVDI9HCa"
   },
   "source": [
    "## Optical Flow Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNrACqel9GLA",
    "outputId": "5daa1ebd-97ca-4445-926e-416ca4839896"
   },
   "outputs": [],
   "source": [
    "# inference on image dir\n",
    "# script from: https://github.com/autonomousvision/unimatch/blob/master/scripts/gmflow_demo.sh\n",
    "!python main_flow.py \\\n",
    "--inference_dir demo/flow-davis \\\n",
    "--resume pretrained/gmflow-scale2-regrefine6-mixdata-train320x576-4e7b215d.pth \\\n",
    "--output_path output/gmflow-scale2-regrefine6-davis \\\n",
    "--padding_factor 32 \\\n",
    "--upsample_factor 4 \\\n",
    "--num_scales 2 \\\n",
    "--attn_splits_list 2 8 \\\n",
    "--corr_radius_list -1 4 \\\n",
    "--prop_radius_list -1 1 \\\n",
    "--reg_refine \\\n",
    "--num_reg_refine 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "RMpU9nAX4rZM",
    "outputId": "6359100e-b2ef-41a3-8c70-ea76bf3c522f"
   },
   "outputs": [],
   "source": [
    "# visualize image and flow\n",
    "img0_path = 'demo/flow-davis/00000.jpg'\n",
    "img1_path = 'demo/flow-davis/00001.jpg'\n",
    "flow_path = 'output/gmflow-scale2-regrefine6-davis/00000_flow.png'\n",
    "\n",
    "img0, img1, flow = plt.imread(img0_path), plt.imread(img1_path), plt.imread(flow_path)\n",
    "vis_img(np.concatenate([img0 / 255., img1 / 255., flow], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optical Flow Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETR_panoptic.ipynb  SETR_panoptic.ipynb  \u001b[0m\u001b[01;34mdemo\u001b[0m/\n",
      "GMFlow.ipynb         UniMatch_demo.ipynb\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arg\n",
      "\u001b[0m\u001b[01;34mmmsegmentation\u001b[0m/  \u001b[01;34msensing\u001b[0m/  \u001b[01;34munimatch\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arg\n",
      "\u001b[0m\u001b[01;34mmmsegmentation\u001b[0m/  \u001b[01;34msensing\u001b[0m/  \u001b[01;34munimatch\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arg/sensing/notebooks\n",
      "DETR_panoptic.ipynb  SETR_panoptic.ipynb  \u001b[0m\u001b[01;34mdemo\u001b[0m/\n",
      "GMFlow.ipynb         UniMatch_demo.ipynb\n"
     ]
    }
   ],
   "source": [
    "%cd notebooks/\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: main_flow.py [-h] [--checkpoint_dir CHECKPOINT_DIR] [--stage STAGE]\n",
      "                    [--val_dataset VAL_DATASET [VAL_DATASET ...]]\n",
      "                    [--max_flow MAX_FLOW]\n",
      "                    [--image_size IMAGE_SIZE [IMAGE_SIZE ...]]\n",
      "                    [--padding_factor PADDING_FACTOR] [--eval]\n",
      "                    [--save_eval_to_file] [--evaluate_matched_unmatched]\n",
      "                    [--val_things_clean_only] [--with_speed_metric] [--lr LR]\n",
      "                    [--batch_size BATCH_SIZE] [--num_workers NUM_WORKERS]\n",
      "                    [--weight_decay WEIGHT_DECAY] [--grad_clip GRAD_CLIP]\n",
      "                    [--num_steps NUM_STEPS] [--seed SEED]\n",
      "                    [--summary_freq SUMMARY_FREQ] [--val_freq VAL_FREQ]\n",
      "                    [--save_ckpt_freq SAVE_CKPT_FREQ]\n",
      "                    [--save_latest_ckpt_freq SAVE_LATEST_CKPT_FREQ]\n",
      "                    [--resume RESUME] [--strict_resume]\n",
      "                    [--no_resume_optimizer] [--task {flow,stereo,depth}]\n",
      "                    [--num_scales NUM_SCALES]\n",
      "                    [--feature_channels FEATURE_CHANNELS]\n",
      "                    [--upsample_factor UPSAMPLE_FACTOR] [--num_head NUM_HEAD]\n",
      "                    [--ffn_dim_expansion FFN_DIM_EXPANSION]\n",
      "                    [--num_transformer_layers NUM_TRANSFORMER_LAYERS]\n",
      "                    [--reg_refine] [--attn_type ATTN_TYPE]\n",
      "                    [--attn_splits_list ATTN_SPLITS_LIST [ATTN_SPLITS_LIST ...]]\n",
      "                    [--corr_radius_list CORR_RADIUS_LIST [CORR_RADIUS_LIST ...]]\n",
      "                    [--prop_radius_list PROP_RADIUS_LIST [PROP_RADIUS_LIST ...]]\n",
      "                    [--num_reg_refine NUM_REG_REFINE] [--gamma GAMMA]\n",
      "                    [--submission] [--output_path OUTPUT_PATH]\n",
      "                    [--save_vis_flow] [--no_save_flo]\n",
      "                    [--inference_dir INFERENCE_DIR]\n",
      "                    [--inference_video INFERENCE_VIDEO]\n",
      "                    [--inference_size INFERENCE_SIZE [INFERENCE_SIZE ...]]\n",
      "                    [--save_flo_flow] [--pred_bidir_flow] [--pred_bwd_flow]\n",
      "                    [--fwd_bwd_check] [--save_video] [--concat_flow_img]\n",
      "                    [--local_rank LOCAL_RANK] [--distributed]\n",
      "                    [--launcher {none,pytorch}]\n",
      "                    [--gpu_ids GPU_IDS [GPU_IDS ...]] [--count_time] [--debug]\n",
      "main_flow.py: error: unrecognized arguments: --save_img\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# directory containing the videos\n",
    "video_dir = \"demo/\"\n",
    "\n",
    "# path to the main_flow.py script\n",
    "main_flow_script = \"/home/arg/unimatch/main_flow.py\"\n",
    "\n",
    "# get a list of all video files in the directory\n",
    "video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n",
    "\n",
    "for video_file in video_files:\n",
    "    # full path to the video file\n",
    "    video_path = os.path.join(video_dir, video_file)\n",
    "    \n",
    "    # output path\n",
    "    output_path = os.path.join(\"demo/output\", os.path.splitext(video_file)[0])\n",
    "    \n",
    "    # command to run the inference script\n",
    "    command = f\"\"\"python3 {main_flow_script} \\\n",
    "    --inference_video \"{video_path}\" \\\n",
    "    --resume pretrained/gmflow-scale2-regrefine6-kitti15-25b554d7.pth \\\n",
    "    --output_path \"{output_path}\" \\\n",
    "    --padding_factor 32 \\\n",
    "    --upsample_factor 4 \\\n",
    "    --num_scales 2 \\\n",
    "    --attn_splits_list 2 8 \\\n",
    "    --corr_radius_list -1 4 \\\n",
    "    --prop_radius_list -1 1 \\\n",
    "    --reg_refine \\\n",
    "    --num_reg_refine 6 \\\n",
    "    --save_img \\\n",
    "    --save_video\"\"\"\n",
    "    \n",
    "    # run the command\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "347p_H-L9KJ7"
   },
   "source": [
    "## Stereo Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y0s-iS0C9L5n",
    "outputId": "4fc584af-0b8b-4c7d-de91-737dbc9c740c"
   },
   "outputs": [],
   "source": [
    "# inference on a stereo pair\n",
    "# script from: https://github.com/autonomousvision/unimatch/blob/master/scripts/gmstereo_demo.sh\n",
    "!python main_stereo.py \\\n",
    "--inference_dir demo/stereo-middlebury \\\n",
    "--inference_size 1024 1536 \\\n",
    "--output_path output/gmstereo-scale2-regrefine3-middlebury \\\n",
    "--resume pretrained/gmstereo-scale2-regrefine3-resumeflowthings-middleburyfthighres-a82bec03.pth \\\n",
    "--padding_factor 32 \\\n",
    "--upsample_factor 4 \\\n",
    "--num_scales 2 \\\n",
    "--attn_type self_swin2d_cross_swin1d \\\n",
    "--attn_splits_list 2 8 \\\n",
    "--corr_radius_list -1 4 \\\n",
    "--prop_radius_list -1 1 \\\n",
    "--reg_refine \\\n",
    "--num_reg_refine 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "YejH62gdFL8r",
    "outputId": "dfef0cff-a7fa-442b-96fa-d51a9840041e"
   },
   "outputs": [],
   "source": [
    "# visualize image and disparity\n",
    "img0_path = 'demo/stereo-middlebury/im0.png'\n",
    "img1_path = 'demo/stereo-middlebury/im1.png'\n",
    "disp_path = 'output/gmstereo-scale2-regrefine3-middlebury/im0_disp.png'\n",
    "\n",
    "img0, img1, disp = plt.imread(img0_path), plt.imread(img1_path), plt.imread(disp_path)\n",
    "vis_img(np.concatenate([img0, img1, disp], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9DAZoPA9MfW"
   },
   "source": [
    "## Depth Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IGp6Yx0CAGd3",
    "outputId": "b6f9f90a-bf92-4123-df32-920312c0295e"
   },
   "outputs": [],
   "source": [
    "# inference on posed images\n",
    "# script from: https://github.com/autonomousvision/unimatch/blob/master/scripts/gmdepth_demo.sh\n",
    "!python main_depth.py \\\n",
    "--inference_dir demo/depth-scannet \\\n",
    "--output_path output/gmdepth-scale1-regrefine1-scannet \\\n",
    "--resume pretrained/gmdepth-scale1-regrefine1-resumeflowthings-scannet-90325722.pth \\\n",
    "--reg_refine \\\n",
    "--num_reg_refine 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "aKt0vY0FGiUA",
    "outputId": "52a10330-5df6-4d02-ba2b-acc2bbc20c0a"
   },
   "outputs": [],
   "source": [
    "# visualize image and depth\n",
    "img0_path = 'demo/depth-scannet/color/0048.png'\n",
    "img1_path = 'demo/depth-scannet/color/0054.png'\n",
    "depth_path = 'output/gmdepth-scale1-regrefine1-scannet/0048.png'\n",
    "\n",
    "img0, img1, depth = plt.imread(img0_path), plt.imread(img1_path), plt.imread(depth_path)\n",
    "vis_img(np.concatenate([img0, img1, depth], axis=1))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
